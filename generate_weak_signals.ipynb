{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from json import JSONDecoder\n",
    "from functools import partial\n",
    "import json\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA & PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def decodeHTMLencoding(tweets):\n",
    "    decoded_tweets = tweets.applymap(lambda tweet: BeautifulSoup(tweet, 'lxml').get_text())\n",
    "    return decoded_tweets\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stopw = stopwords.words('english')\n",
    "    words = [word for word in text.split() if len(word) > 3 and not word in stopw]\n",
    "    # get stems from words\n",
    "    for i in range(len(words)):\n",
    "        words[i] = stemmer.stem(words[i])\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    # decode tweets from html tags\n",
    "    cleaned_tweets = decodeHTMLencoding(tweets)\n",
    "    # remove URLs that starts with http\n",
    "    cleaned_tweets = cleaned_tweets.applymap(lambda tweet: re.sub(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove URLs that does not start with http\n",
    "    cleaned_tweets = cleaned_tweets.applymap(lambda tweet: re.sub(\n",
    "    r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE))\n",
    "    # remove @\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'@[A-Za-z0-9_]+', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove #\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'#[A-Za-z0-9_]+', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove RT\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub('RT ', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove symbols and numbers (i.e keep letters only)\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(\"[^a-zA-Z]\",\" \",tweet, flags=re.MULTILINE) )\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'[^\\x00-\\x7F]+',\" \",tweet.lower(), flags=re.MULTILINE) )\n",
    "    \n",
    "    cleaned_tweets.drop_duplicates(inplace=True)\n",
    "    cleaned_tweets.replace('', np.nan, inplace=True)\n",
    "    cleaned_tweets.dropna(inplace=True)\n",
    "    \n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(tweets, model):\n",
    "    # dataset should be a pandas dataframe\n",
    "    dimension = 300\n",
    "    data_array = np.empty(shape=[0, dimension])\n",
    "    indexes = []\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        words = tweet.split()\n",
    "        if len(words) !=0:\n",
    "            feature = 0\n",
    "            for word in words:\n",
    "                try:\n",
    "                    feature += model[word]\n",
    "                except:\n",
    "                    pass\n",
    "            feature /= len(words)\n",
    "            try:\n",
    "                if feature.size == dimension:  \n",
    "                    data_array = np.append(data_array, [feature], axis=0)\n",
    "                    indexes.append(i)\n",
    "            except:\n",
    "                continue\n",
    "    indexes = np.asarray(indexes)\n",
    "    assert indexes.size == data_array.shape[0]\n",
    "    return data_array, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indices(weak_signals):\n",
    "    # remove indexes of weak_signals that do not have coverage\n",
    "    indices = np.where(np.sum(weak_signals, axis=1) == -1*weak_signals.shape[1])[0]\n",
    "    weak_signals = np.delete(weak_signals, indices, axis=0)\n",
    "    \n",
    "    return weak_signals, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/glove.42B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_model = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358371614102348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test word vectors\n",
    "from scipy import spatial\n",
    "result = 1 - spatial.distance.cosine(glove_model['horrible'], glove_model['terrible'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_labeling(data, keywords, sentiment='pos'):\n",
    "    mask = 1 if sentiment == 'pos' else 0\n",
    "    weak_signals = []\n",
    "    for terms in keywords:\n",
    "        weak_signal = []\n",
    "        for text in data:\n",
    "            label=-1\n",
    "            for word in terms:\n",
    "                if word in text.lower():\n",
    "                    label = mask\n",
    "            weak_signal.append(label)\n",
    "        weak_signals.append(weak_signal)\n",
    "    return np.asarray(weak_signals).T\n",
    "\n",
    "POSITIVE_LABELS =  [['good','great','nice','delight','wonderful'], \n",
    "                    ['love', 'best', 'genuine','well', 'thriller'], \n",
    "                    ['clever','enjoy','fine','deliver','fascinating'], \n",
    "                    ['super','excellent','charming','pleasure','strong'], \n",
    "                    ['fresh','comedy', 'interesting','fun','entertain', 'charm', 'clever'], \n",
    "                    ['amazing','romantic','intelligent','classic','stunning'],\n",
    "                    ['rich','compelling','delicious', 'intriguing','smart']]\n",
    "\n",
    "NEGATIVE_LABELS = [['bad','better','leave','never','disaster'], \n",
    "                   ['nothing','action','fail','suck','difficult'], \n",
    "                   ['mess','dull','dumb', 'bland','outrageous'], \n",
    "                   ['slow', 'terrible', 'boring', 'insult','weird','damn'],\n",
    "                   ['drag','awful','waste', 'flat','worse'],\n",
    "                   #['drag','no','not','awful','waste', 'flat'], \n",
    "                   ['horrible','ridiculous','stupid', 'annoying','painful'], \n",
    "                   ['poor','pathetic','pointless','offensive','silly']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/yelp/'\n",
    "size = 10000\n",
    "review = pd.read_json(datapath+'yelp_review.json', lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple chunks to be read\n",
    "count=0\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['review_id','user_id','useful','funny','cool','business_id','date'], axis=1)\n",
    "    chunk_list.append(chunk_review)\n",
    "    count +=1\n",
    "    if count==6:\n",
    "        break\n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      2  As someone who has worked with many museums, I...\n",
       "1      1  I am actually horrified this place is still in...\n",
       "2      5  I love Deagan's. I do. I really do. The atmosp...\n",
       "3      1  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...\n",
       "4      4  Oh happy day, finally have a Canes near my cas..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = datapath+\"yelp_reviews.csv\"\n",
    "df.to_csv(csv_name, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55392, 14)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_labels = keyword_labeling(df.text.values, POSITIVE_LABELS, sentiment='pos')\n",
    "negative_labels = keyword_labeling(df.text.values, NEGATIVE_LABELS, sentiment='neg')\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals, indices = remove_indices(weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[indices])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "train_data = df.text.values\n",
    "train_labels = np.zeros(df.shape[0])\n",
    "train_labels[df.stars.values >3]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55370, 1), (55370,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = cleanTweets(df.drop(columns=['stars']))\n",
    "train_labels = train_labels[train_data.index]\n",
    "weak_signals = weak_signals[train_data.index]\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_index = get_text_vectors(train_data.values.ravel(), glove_model)\n",
    "train_features.shape, train_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test data\n",
    "np.random.seed(5000)\n",
    "test_indexes = np.random.choice(train_index.size, 10000, replace=False)\n",
    "test_labels = train_labels[test_indexes]\n",
    "test_data = train_features[test_indexes]\n",
    "\n",
    "train_data = np.delete(train_features, test_indexes, axis=0)\n",
    "weak_signals = np.delete(weak_signals, test_indexes, axis=0)\n",
    "train_labels = np.delete(train_labels, test_indexes)\n",
    "\n",
    "train_data.shape,train_labels.shape,weak_signals.shape,test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weak_signals signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals)\n",
    "\n",
    "# save yelp data\n",
    "np.save(datapath+'data_features.npy', train_data)\n",
    "np.save(datapath+'test_features.npy', test_data)\n",
    "\n",
    "# save yelp labels\n",
    "np.save(datapath+'data_labels.npy', train_labels)\n",
    "np.save(datapath+'test_labels.npy', test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45370, 300), (45370,), (45370, 14), (10000,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape,train_labels.shape,weak_signals.shape,test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A stirring, funny and finally transporting re-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Apparently reassembled from the cutting-room f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>They presume their audience won't sit still fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a visually stunning rumination on love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Jonathan Parker's Bartleby should have been th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  A stirring, funny and finally transporting re-...\n",
       "1      0  Apparently reassembled from the cutting-room f...\n",
       "2      0  They presume their audience won't sit still fo...\n",
       "3      1  This is a visually stunning rumination on love...\n",
       "4      1  Jonathan Parker's Bartleby should have been th..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/sst-2/'\n",
    "train_data = pd.read_csv(datapath+'sst2-train.csv')\n",
    "test_data = pd.read_csv(datapath+'sst2-test.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEGATIVE_LABELS = [['bad','better','leave','never','disaster'], \n",
    "                   ['nothing','action','fail','suck','difficult'], \n",
    "                   ['mess','dull','dumb', 'bland','outrageous'], \n",
    "                   ['slow', 'terrible', 'boring', 'insult','weird','damn'],\n",
    "                   # ['drag','awful','waste', 'flat','worse'],\n",
    "                   ['drag','no','not','awful','waste', 'flat'], \n",
    "                   ['horrible','ridiculous','stupid', 'annoying','painful'], \n",
    "                   ['poor','pathetic','pointless','offensive','silly']]\n",
    "\n",
    "positive_labels = keyword_labeling(train_data.sentence.values, POSITIVE_LABELS)\n",
    "negative_labels = keyword_labeling(train_data.sentesnce.values, NEGATIVE_LABELS, sentiment='neg')\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_signals, indices = remove_indices(train_data, weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  [[0.30916844]\n",
      " [0.29194631]\n",
      " [0.26710098]\n",
      " [0.29081633]\n",
      " [0.36492375]\n",
      " [0.31952663]\n",
      " [0.19417476]\n",
      " [0.34623218]\n",
      " [0.32853026]\n",
      " [0.2513369 ]\n",
      " [0.33333333]\n",
      " [0.44829801]\n",
      " [0.15116279]\n",
      " [0.18348624]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.label.values\n",
    "test_labels = test_data.label.values\n",
    "\n",
    "n,m = weak_signals.shape\n",
    "weak_signal_probabilities = weak_signals.T.reshape(m,n,1)\n",
    "\n",
    "weak_signals_mask = weak_signal_probabilities >=0\n",
    "\n",
    "from model_utilities import get_error_bounds\n",
    "true_error_rates = get_error_bounds(train_labels, weak_signal_probabilities, weak_signals_mask)\n",
    "print(\"error: \", np.asarray(true_error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3998, 1) (3998,)\n",
      "(1821, 1) (1821,)\n"
     ]
    }
   ],
   "source": [
    "# Clean data and reset index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# apply on train data\n",
    "train_data = cleanTweets(train_data.drop(columns=['label']))\n",
    "train_data = post_process_tweets(train_data)\n",
    "\n",
    "# apply on test data\n",
    "test_data = cleanTweets(test_data.drop(columns=['label']))\n",
    "test_data = post_process_tweets(test_data)\n",
    "\n",
    "print(train_data[0].shape, train_labels.shape)\n",
    "print(test_data[0].shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a stirring  funny and finally transporting re ...\n",
       "1       they presume their audience won t sit still fo...\n",
       "2       this is a visually stunning rumination on love...\n",
       "3       campanella gets the tone just right    funny i...\n",
       "4       a fan film that for the uninitiated plays bett...\n",
       "                              ...                        \n",
       "3600    painful  horrifying and oppressively tragic  t...\n",
       "3601    take care is nicely performed by a quintet of ...\n",
       "3602    the script covers huge  heavy topics in a blan...\n",
       "3603    a seriously bad film with seriously warped log...\n",
       "3604    a deliciously nonsensical comedy about a city ...\n",
       "Name: sentence, Length: 3605, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_index = get_text_vectors(train_data[0].values.ravel(), glove_model)\n",
    "test_features, test_index = get_text_vectors(test_data[0].values.ravel(), glove_model)\n",
    "\n",
    "# save sst-2 data\n",
    "np.save(datapath+'data_features.npy', train_features)\n",
    "np.save(datapath+'test_features.npy', test_features)\n",
    "\n",
    "indexes = train_data[1]\n",
    "indexes = indexes[train_index]\n",
    "# save sst-2 labels\n",
    "np.save(datapath+'data_labels.npy', train_labels[indexes])\n",
    "np.save(datapath+'test_labels.npy', test_labels[test_data[1]])\n",
    "\n",
    "# save the one-hot signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), 49580)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/imdb/'\n",
    "df = pd.read_csv(datapath+'IMDB Dataset.csv')\n",
    "\n",
    "# apply on train data\n",
    "cleaned_data = cleanTweets(df.drop(columns=['sentiment']))\n",
    "indexes = cleaned_data.index.values\n",
    "df.shape, indexes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39664,) (39664,)\n",
      "(9916,) (9916,)\n"
     ]
    }
   ],
   "source": [
    "n = indexes.size\n",
    "# get test data\n",
    "np.random.seed(50)\n",
    "test_indexes = np.random.choice(indexes, int(n*0.2), replace=False)\n",
    "test_labels = np.zeros(test_indexes.size)\n",
    "test_labels[df.sentiment.values[test_indexes]=='positive'] = 1\n",
    "test_data = df.review.values[test_indexes]\n",
    "\n",
    "train_indexes = np.delete(indexes, [np.where(indexes == i)[0][0] for i in test_indexes])\n",
    "train_labels = np.zeros(train_indexes.size)\n",
    "train_labels[df.sentiment.values[train_indexes]=='positive'] = 1\n",
    "train_data = df.review.values[train_indexes]\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29187, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_labels = keyword_labeling(train_data, [['good'],['wonderful'],['great'],['amazing'],['excellent']], sentiment='pos')\n",
    "negative_labels = keyword_labeling(train_data, [['bad'],['horrible'],['sucks'],['awful'],['terrible']], sentiment='neg')\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals, indices = remove_indices(weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29182 29187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20392, 20393)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add signals not covered to test data\n",
    "test_data = np.append(test_data, train_data[indices])\n",
    "test_labels = np.append(test_labels, train_labels[indices])\n",
    "\n",
    "# delete train data not covered by weak signals\n",
    "train_data = np.delete(train_data, indices, axis=0)\n",
    "train_labels = np.delete(train_labels, indices)\n",
    "\n",
    "# get data features\n",
    "train_features, train_index = get_text_vectors(train_data, glove_model)\n",
    "test_features, test_index = get_text_vectors(test_data, glove_model)\n",
    "\n",
    "print(train_index.size, train_data.shape[0])\n",
    "test_index.size, test_labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save imdb data\n",
    "np.save(datapath+'data_features.npy', train_features)\n",
    "np.save(datapath+'test_features.npy', test_features)\n",
    "\n",
    "# save imdb labels\n",
    "np.save(datapath+'data_labels.npy', train_labels[train_index])\n",
    "np.save(datapath+'test_labels.npy', test_labels[test_index])\n",
    "\n",
    "# save the weak_signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals[train_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
